{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDRegressor, LinearRegression, LogisticRegression, Lasso, Ridge\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error, r2_score\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FrankeFunctionWithNoise(x,y):\n",
    "    term1 = 0.75*np.exp(-(0.25*(9*x-2)**2) - 0.25*((9*y-2)**2))\n",
    "    term2 = 0.75*np.exp(-((9*x+1)**2)/49.0 - 0.1*(9*y+1))\n",
    "    term3 = 0.5*np.exp(-(9*x-7)**2/4.0 - 0.25*((9*y-3)**2))\n",
    "    term4 = -0.2*np.exp(-(9*x-4)**2 - (9*y-7)**2)\n",
    "    noise = np.random.normal(0, 0.1, len(x)*len(x))\n",
    "    noise = noise.reshape(len(x),len(x))\n",
    "    return term1 + term2 + term3 + term4 + noise\n",
    "\n",
    "def create_X(x, y, n ):\n",
    "    if len(x.shape) > 1:\n",
    "        x = np.ravel(x)\n",
    "        y = np.ravel(y)\n",
    "    N = len(x)\n",
    "    l = int((n+1)*(n+2)/2) # Number of elements in beta\n",
    "    X = np.ones((N,l))\n",
    "\n",
    "    for i in range(1,n+1):\n",
    "        q = int((i)*(i+1)/2)\n",
    "    return X\n",
    "\n",
    "def plotFunction(x, y, z, title):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "    # Plot the surface\n",
    "    surf = ax.plot_surface(x, y, z, cmap=cm.coolwarm,linewidth=0, antialiased=False)\n",
    "    # Customize the z axis.\n",
    "    ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "    ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "    # Add a color bar which maps values to colors.\n",
    "    fig.suptitle(title)\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Represents a layer (hidden or output) in our neural network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_input, n_neurons, activation=None, alpha=0.01, lam=0.1):\n",
    "        \"\"\"\n",
    "        :param int n_input: The input size (coming from the input layer or a previous hidden layer)\n",
    "        :param int n_neurons: The number of neurons in this layer.\n",
    "        :param str activation: The activation function to use (if any).\n",
    "        :param weights: The layer's weights.\n",
    "        :param bias: The layer's bias.\n",
    "        \"\"\"\n",
    "        self.activation = activation\n",
    "        self.alpha = alpha\n",
    "        self.lam = lam\n",
    "        self.last_activation = None\n",
    "        self.error = None\n",
    "        self.delta = None\n",
    "        # self.weights = weights if weights is not None else np.random.rand(n_input, n_neurons)\n",
    "        # self.activation = activation\n",
    "        # self.bias = bias if bias is not None else np.random.rand(n_neurons)\n",
    "\n",
    "        np.random.seed(1921)\n",
    "        # Xavier initializations (http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf).\n",
    "        if self.activation == 'sigmoid':\n",
    "            r_inputs = np.sqrt(6.0 / (n_input + n_neurons))\n",
    "            self.weights = np.random.uniform(-r_inputs, r_inputs, size=(n_input, n_neurons))\n",
    "            # self.weights = np.random.rand(n_input, n_neurons)\n",
    "            self.bias = np.random.rand(n_neurons)\n",
    "            # self.v_reg = np.random.rand(n_neurons, 1)\n",
    "            self.v_reg = np.random.uniform(-(6 / (n_neurons + 1)), (6 / (n_neurons + 1)), size=(n_neurons, 1))\n",
    "            self.bias_reg = np.random.rand(1)\n",
    "\n",
    "        elif self.activation == 'tanh':\n",
    "            r_inputs = 4.0 * np.sqrt(6.0 / (n_input + n_neurons))\n",
    "            self.weights = np.random.uniform(-r_inputs, r_inputs, size=(n_input, n_neurons))\n",
    "            # self.weights = np.random.rand(n_input, n_neurons)\n",
    "            self.v_reg = np.random.uniform(-4.0 * np.sqrt(6.0 / (n_neurons + 1)), 4.0 * np.sqrt(6.0 / (n_neurons + 1)),\n",
    "                                           size=(n_neurons, 1))\n",
    "            # self.v_reg = np.random.rand(n_neurons, 1)\n",
    "            # self.bias = np.zeros(shape=(n_neurons))\n",
    "            self.bias = np.random.rand(n_neurons)\n",
    "            self.bias_reg = np.random.rand(1)  ##output bias\n",
    "\n",
    "        # He initializations (https://arxiv.org/pdf/1502.01852.pdf).\n",
    "        elif self.activation == 'relu' or self.activation == 'leaky_relu' or self.activation == 'elu':\n",
    "            self.weights = np.random.normal(size=(n_input, n_neurons)) * np.sqrt(2.0 / n_input)\n",
    "            # self.weights = np.random.rand(n_input, n_neurons)\n",
    "            # self.v_reg = np.random.rand(n_neurons, 1)\n",
    "            self.v_reg = np.random.normal(size=(n_neurons, 1)) * np.sqrt(2.0 / n_neurons)\n",
    "            self.bias = np.random.rand(n_neurons)\n",
    "            self.bias_reg = np.random.rand(1)\n",
    "\n",
    "        else:\n",
    "            self.weights = np.random.normal(size=(n_input, n_neurons))\n",
    "            # self.weights = np.random.rand(n_input, n_neurons)\n",
    "            # self.v_reg = np.random.rand(n_neurons, 1)\n",
    "            self.v_reg = np.random.normal(size=(n_neurons, 1))\n",
    "            self.bias = np.random.rand(n_neurons)\n",
    "            self.bias_reg = np.random.rand(1)\n",
    "\n",
    "    def activate(self, x):\n",
    "        \"\"\"\n",
    "        Calculates the dot product of this layer.\n",
    "        :param x: The input.\n",
    "        :return: The result.\n",
    "        \"\"\"\n",
    "\n",
    "        r = np.dot(x, self.weights) + self.bias\n",
    "        self.last_activation = self._apply_activation(r)\n",
    "        return self.last_activation\n",
    "\n",
    "    def _apply_activation(self, r):\n",
    "        \"\"\"\n",
    "        Applies the chosen activation function (if any).\n",
    "        :param r: The normal value.\n",
    "        :return: The activated value.\n",
    "        \"\"\"\n",
    "\n",
    "        # In case no activation function was chosen\n",
    "\n",
    "        if self.activation is None:\n",
    "            return r\n",
    "\n",
    "        if self.activation == 'tanh':\n",
    "            return np.tanh(r)\n",
    "\n",
    "        if self.activation == 'sigmoid':\n",
    "            return self._sigmoid(r)\n",
    "\n",
    "        if self.activation == 'relu':\n",
    "            return self._relu(r)\n",
    "\n",
    "        if self.activation == 'leaky_relu':\n",
    "            return self._leakyrelu(r)\n",
    "\n",
    "        if self.activation == 'elu':\n",
    "            return self._elu(r)\n",
    "\n",
    "        if self.activation == 'softmax':\n",
    "            return self._softmax(r)\n",
    "\n",
    "        if self.activation == 'identity':\n",
    "            return self._identity(r)\n",
    "\n",
    "        return r\n",
    "\n",
    "    def apply_activation_derivative(self, r):\n",
    "        \"\"\"\n",
    "        Applies the derivative of the activation function (if any).\n",
    "        :param r: The normal value.\n",
    "        :return: The \"derived\" value.\n",
    "        \"\"\"\n",
    "\n",
    "        # We use 'r' directly here because its already activated, the only values that\n",
    "\n",
    "        # are used in this function are the last activations that were saved.\n",
    "\n",
    "        if self.activation is None:\n",
    "            return r\n",
    "\n",
    "        if self.activation == 'tanh':\n",
    "            return 1 - r ** 2\n",
    "\n",
    "        if self.activation == 'sigmoid':\n",
    "            return r * (1 - r)\n",
    "\n",
    "        if self.activation == 'relu':\n",
    "            r[r > 0] = self.lam\n",
    "            return r\n",
    "\n",
    "        if self.activation == 'leaky_relu':\n",
    "            r[r > 0] = self.lam\n",
    "            r[r <= 0] = self.lam * self.alpha\n",
    "            return r\n",
    "\n",
    "        if self.activation == 'identity':\n",
    "            return 1\n",
    "\n",
    "        if self.activation == 'elu':\n",
    "            r[r > 0] = 1\n",
    "            r[r <= 0] = r[r <= 0] + self.alpha\n",
    "\n",
    "        return r\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def _tanh(self, x):\n",
    "\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def _relu(self, x):\n",
    "\n",
    "        x = self.lam * x\n",
    "        x[x <= 0] = 0\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _leakyrelu(self, x):\n",
    "\n",
    "        x = self.lam * x\n",
    "        x[x <= 0] = self.alpha * x[x <= 0]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _identity(self, x):  ##linear activation function\n",
    "        return x\n",
    "\n",
    "    def _elu(self, x):\n",
    "        neg = x < 0.0\n",
    "        x[neg] = self.alpha * (np.exp(x[neg]) - 1.0)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _softmax(self, x):\n",
    "        exps = np.exp(x - np.max(x))\n",
    "        return exps / np.sum(exps, axis=0, keepdims=True)  ## sum along the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Represents a neural network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._layers = []\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        \"\"\"\n",
    "        Adds a layer to the neural network.\n",
    "        :param Layer layer: The layer to add.\n",
    "        \"\"\"\n",
    "\n",
    "        self._layers.append(layer)\n",
    "\n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Feed forward the input through the layers.\n",
    "        :param X: The input values.\n",
    "        :return: The result.\n",
    "        \"\"\"\n",
    "\n",
    "        for layer in self._layers:\n",
    "            X = layer.activate(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "        \"\"\"\n",
    "        N.B: Having a sigmoid activation in the output layer can be interpreted\n",
    "        as expecting probabilities as outputs.\n",
    "        W'll need to choose a winning class, this is usually done by choosing the\n",
    "        index of the biggest probability.\n",
    "        \"\"\"\n",
    "\n",
    "    def predict(self, X, net_type='regression', n_neurons=3):\n",
    "        \"\"\"\n",
    "        Predicts a class (or classes).\n",
    "        :param X: The input values.\n",
    "        :return: The predictions.\n",
    "         \"\"\"\n",
    "\n",
    "        ff = self.feed_forward(X)\n",
    "\n",
    "        if net_type == 'classification':\n",
    "\n",
    "            # One row\n",
    "\n",
    "            if ff.ndim == 1:\n",
    "                pred = np.argmax(ff)\n",
    "            else:\n",
    "                pred = np.argmax(ff, axis=1)\n",
    "\n",
    "        if net_type == 'regression':\n",
    "            pred = ff\n",
    "\n",
    "        return pred\n",
    "\n",
    "        # # Multiple rows\n",
    "\n",
    "        # return np.argmax(ff, axis=1)\n",
    "\n",
    "    def backpropagation(self, X, y, learning_rate, lmbd, net_type='classification'):\n",
    "        \"\"\"\n",
    "        Performs the backward propagation algorithm and updates the layers weights.\n",
    "        :param X: The input values.\n",
    "        :param y: The target values.\n",
    "        :param float learning_rate: The learning rate (between 0 and 1).\n",
    "        \"\"\"\n",
    "        ntarget = y.size\n",
    "\n",
    "        # Feed forward for the output\n",
    "\n",
    "        output = self.feed_forward(X)\n",
    "\n",
    "        # Loop over the layers backward\n",
    "\n",
    "        for i in reversed(range(len(self._layers))):\n",
    "            layer = self._layers[i]\n",
    "\n",
    "            # If this is the output layer\n",
    "            if layer == self._layers[-1]:\n",
    "                layer.error = y - output\n",
    "\n",
    "                # The output = layer.last_activation in this case\n",
    "                layer.delta = layer.error * layer.apply_activation_derivative(output)\n",
    "\n",
    "            else:\n",
    "                next_layer = self._layers[i + 1]\n",
    "                layer.error = np.dot(next_layer.weights, next_layer.delta)\n",
    "                layer.delta = layer.error * layer.apply_activation_derivative(layer.last_activation)\n",
    "\n",
    "        # Update the weights\n",
    "\n",
    "        for i in range(len(self._layers)):\n",
    "            layer = self._layers[i]\n",
    "            # The input is either the previous layers output or X itself (for the first hidden layer)\n",
    "\n",
    "            input_to_use = np.atleast_2d(X if i == 0 else self._layers[i - 1].last_activation)\n",
    "\n",
    "            layer.weights = layer.weights + layer.delta * input_to_use.T * learning_rate\n",
    "\n",
    "            if lmbd > 0:  ###adding L2 regularization\n",
    "\n",
    "                layer.weights = layer.weights * (\n",
    "                            1 - lmbd * learning_rate) + layer.delta * input_to_use.T * learning_rate\n",
    "\n",
    "            layer.bias = layer.bias + layer.delta * learning_rate\n",
    "\n",
    "    def train(self, X, y, learning_rate, max_epochs, net_type='regression', lmbd=0):\n",
    "        \"\"\"\n",
    "        Trains the neural network using backpropagation.\n",
    "        :param X: The input values.\n",
    "        :param y: The target values.\n",
    "        :param float learning_rate: The learning rate (between 0 and 1).\n",
    "        :param int max_epochs: The maximum number of epochs (cycles).\n",
    "        :return: The list of calculated MSE errors.\n",
    "        \"\"\"\n",
    "\n",
    "        mses = []\n",
    "\n",
    "        for i in range(max_epochs):\n",
    "            for j in range(len(X)):  ##len(X) is rows\n",
    "                self.backpropagation(X[j], y[j], learning_rate, lmbd)\n",
    "\n",
    "            # if i % 10 == 0: #At every 10th epoch, we will print out the Mean Squared Error and save it in mses which we will return at the end.\n",
    "            #     nn = NeuralNetwork()\n",
    "            #     mse = np.mean(np.square(y - nn.feed_forward(X)))\n",
    "            #     mses.append(mse)\n",
    "            #     print('Epoch: #%s, MSE: %f' % (i, float(mse)))\n",
    "            # return mses\n",
    "\n",
    "    def MSE(self, y_pred, y_true):\n",
    "        return (1 / len(y_true)) * np.sum((y_pred - y_true) ** 2)\n",
    "\n",
    "    def accuracy(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Calculates the accuracy between the predicted labels and true labels.\n",
    "        :param y_pred: The predicted labels.\n",
    "        :param y_true: The true labels.\n",
    "        :return: The calculated accuracy.\n",
    "        \"\"\"\n",
    "\n",
    "        return (y_pred == y_true).mean()\n",
    "\n",
    "    def cal_err(self, y_pred, y_true, costf):\n",
    "\n",
    "        if costf == \"squared-error\":\n",
    "            err = np.sum((y_pred - y_true) ** 2)\n",
    "        elif costf == \"MSE\":\n",
    "            err = (1 / len(y_true)) * np.sum((y_pred - y_true) ** 2)\n",
    "\n",
    "        return err\n",
    "\n",
    "    def confusion_table(self, y_pred, y_true):\n",
    "\n",
    "        conf = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "\n",
    "        return conf\n",
    "\n",
    "    def cal_r2(self, y_pred, y_true):\n",
    "\n",
    "        mu = np.mean(y_true)\n",
    "        SS_tot = np.sum((y_true - mu) ** 2)\n",
    "        SS_res = np.sum((y_true - y_pred) ** 2)\n",
    "\n",
    "        r2 = 1 - (SS_res / SS_tot)\n",
    "\n",
    "        return r2\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
